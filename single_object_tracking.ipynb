{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Object Tracking \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca76625c3f44da694419f7565ef8b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=1, description='Which Sequence of Images:', max=5, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Initialization function to load data and create slider to update the path of sequence of images\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Creating a slider to choose which sequence images to test (ranging from 1 to 5 with default value is 1)\n",
    "seq_slider = widgets.IntSlider(value=1, min=1, max=5, description='Which Sequence of Images:')\n",
    "display(seq_slider) # to display the slider in the terminal output\n",
    "\n",
    "# implement function to update the path based on selected sequence of images\n",
    "def update_paths(seq_number):\n",
    "    # directory and file for sequence of images, firsttrack and groundtruth data\n",
    "    image_seq = f\"data/Task 1/seq_{seq_number}/img\"\n",
    "    firsttrack_seq = f\"data/Task 1/seq_{seq_number}/firsttrack.txt\"\n",
    "    groundtruth_seq = f\"data/Task 1/seq_{seq_number}/groundtruth.txt\"\n",
    "    return image_seq, firsttrack_seq, groundtruth_seq\n",
    "\n",
    "# implement function to update the path when the slider value changes\n",
    "def update_slider(change):\n",
    "    global image_seq, firsttrack_seq, groundtruth_seq\n",
    "    global image_sequences, template_coords, ground_truth_coords\n",
    "    \n",
    "    image_seq, firsttrack_seq, groundtruth_seq = update_paths(change.new) # this will call update_paths function with a new slider value\n",
    "    image_sequences = read_sequenceimage(image_seq) # used to load and read sequence of images by calling function read_sequenceimage\n",
    "    template_coords = read_firsttrack(firsttrack_seq) # used to read firsttrack data by calling function read_firsttrack\n",
    "    ground_truth_coords = read_groundtruth(groundtruth_seq) # used to read ground truth data by calling function read_groundtruth\n",
    "\n",
    "# implement function to load and read sequence images by using \"cv2.imread\"\n",
    "def read_sequenceimage(image_path):\n",
    "    image_seq = []\n",
    "    image_files = sorted(glob.glob(os.path.join(os.getcwd(), image_path, '*.jpg' ))) # sorted all images path into image_files list\n",
    "\n",
    "    # loop into each path of image and read by \"cv2.imread\" and store it into image_seq list\n",
    "    for image_file in image_files:\n",
    "        image = cv2.imread(image_file)\n",
    "        image_seq.append(image)\n",
    "    return image_seq\n",
    "\n",
    "# implement function to read groundtruth data\n",
    "def read_groundtruth(ground_path):\n",
    "    with open(ground_path, 'r') as file:\n",
    "        lines = file.readlines() # read line by line and store it into lines variable\n",
    "        ground_coords = [list(map(int, line.strip().split(','))) for line in lines] # write ground truth coordinates that split by \",\" from line to line into ground_coords list\n",
    "    return ground_coords\n",
    "    \n",
    "# implement function to read firstract data\n",
    "def read_firsttrack(firsttrack_path):\n",
    "    with open(firsttrack_path, 'r') as file:\n",
    "        lines = file.readlines()  # read line by line and store it into lines variable\n",
    "        template_coords = [list(map(int, line.strip().split(','))) for line in lines][0] # write firsttrack coordinates that split by \",\" from line to line into template_coords list\n",
    "    return template_coords\n",
    "\n",
    "# load data to variable based on given function\n",
    "image_seq, firsttrack_seq, groundtruth_seq = update_paths(seq_slider.value) # this will call update_paths function with a selected value from slider\n",
    "image_sequences = read_sequenceimage(image_seq)\n",
    "template_coords = read_firsttrack(firsttrack_seq)\n",
    "ground_truth_coords = read_groundtruth(groundtruth_seq)\n",
    "\n",
    "# update data to variable if the slider value changed\n",
    "seq_slider.observe(update_slider, names=\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Template Matching Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## template matching algorithm\n",
    "\n",
    "def template_matching_algorithm(image_sequences, template_coords, ground_truth_coords):\n",
    "    \n",
    "    image_results = [] # use to store the result of tracking object on the image\n",
    "    template_coords_results = [] # use to store the coordinate of boundary box for template object\n",
    "    template_coords_results.append(template_coords)\n",
    "    update_interval = 50 # used for update the template coordinate using the data from groundtruth\n",
    "\n",
    "    template = image_sequences[0][template_coords[1]:template_coords[1] + template_coords[3],\n",
    "                    template_coords[0]:template_coords[0] + template_coords[2]] # cropping the first image by boundary box coordinate to get the template image\n",
    "\n",
    "    # looping through all of sequence of images to do single object tracking\n",
    "    for i in range(1, len(image_sequences)):\n",
    "        current_frame = image_sequences[i] # obtain current image\n",
    "\n",
    "        if i % update_interval == 0: # condition to update the template coordinate based on groundtruth data\n",
    "            template_coords = ground_truth_coords[i] # update template coordinates from selected ground truth data\n",
    "\n",
    "            template = current_frame[template_coords[1]:template_coords[1] + template_coords[3], \n",
    "                                     template_coords[0]:template_coords[0] + template_coords[2]]\n",
    "            \n",
    "        result = cv2.matchTemplate(current_frame, template, cv2.TM_CCOEFF_NORMED) # use cv2.matchTemplate to match between template with corresponding image \n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result) # get the min and max location as well as the value from cv2.minMaxLoc\n",
    "\n",
    "        template_coords = [max_loc[0], max_loc[1],template_coords[2],template_coords[3]] # update template coordinates by using max loc for x and y, and width and height from previous template\n",
    "\n",
    "        template = current_frame[template_coords[1]:template_coords[1] + template_coords[3], template_coords[0]:template_coords[0] + template_coords[2]] # update the template image by using current template coordinates\n",
    "\n",
    "        template_coords_results.append(template_coords) # append the template coordinates (the boundary box of the target object)\n",
    "\n",
    "        current_frame_result = current_frame.copy()\n",
    "\n",
    "        cv2.rectangle(current_frame_result,(template_coords[0],template_coords[1]),(template_coords[0] + template_coords[2], template_coords[1] + template_coords[3]),\n",
    "                        (0, 255, 0), 2) # draw tracked object boundary box in current image using cv2.rectangle\n",
    "        \n",
    "        cv2.putText(current_frame_result, \"Template Matching\", (template_coords[0],template_coords[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2) # put text name of \"Template Matching\" in the upper of boundary box of tracked object\n",
    "\n",
    "        cv2.rectangle(current_frame_result,(ground_truth_coords[i][0],ground_truth_coords[i][1]),(ground_truth_coords[i][0] + ground_truth_coords[i][2], ground_truth_coords[i][1] + ground_truth_coords[i][3]),\n",
    "                        (0, 0, 255), 2) # draw ground truth boundary box in current image using cv2.rectangle\n",
    "        \n",
    "        cv2.putText(current_frame_result, \"Ground Truth\", (ground_truth_coords[i][0],ground_truth_coords[i][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2) # put text name of \"Ground Truth\" in the upper of boundary box of ground truth\n",
    "        \n",
    "        image_results.append(current_frame_result) # append the result image that have boundary box of target object\n",
    "    \n",
    "    return image_results, template_coords_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Kalman Filter Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kalman Filter Implementation\n",
    "\n",
    "# this function used to implement the calculation step of kalman filter (prediction state, correction step and update step)\n",
    "def kalman_filter_implementation(initial_state, F, H, P, R, Q, measurement):\n",
    "\n",
    "    state = np.array(initial_state, dtype=np.float32).reshape(-1,1) # used to reshape initial state to corresponding vector size\n",
    "    F = np.array(F, dtype=np.float32) # State transition matrix\n",
    "    H = np.array(H, dtype=np.float32) # Measurement matrix\n",
    "    P = np.array(P, dtype=np.float32) # State transition covariance matrix (predicted error covariance matrix)\n",
    "    R = np.array(R, dtype=np.float32) # Measurement noise covariance matrix\n",
    "    Q = np.array(Q, dtype=np.float32) # Process noise covariance matrix\n",
    "\n",
    "    # Start with prediction step\n",
    "    state = F @ state # not used control input (u) as there is no data input for control input as well as gausian noise (w)\n",
    "    P = F @ P @ F.T + Q # compute predicted state covariance matrix\n",
    "\n",
    "    # Correction and update step using measurement from groundtruth data\n",
    "    measurement = np.array(measurement, dtype=np.float32).reshape(-1,1) # used to reshape measurement into corresponding vector size\n",
    "    y = measurement - H @ state\n",
    "    S = H @ P @ H.T + R # calculate measurement covariance matrix\n",
    "    K = P @ H.T @ np.linalg.inv(S) # calculate kalman gain\n",
    "    state += K @ y # update step for state\n",
    "    P = (np.eye(4) - K @ H) @ P # update step for state covariance matrix\n",
    "\n",
    "    return state.flatten()\n",
    "\n",
    "# this function use to do single object tracking by using kalman filter method\n",
    "def kalman_filter_algorithm(initial_state, image_sequences, groundtruth_data):\n",
    "\n",
    "    image_results = [] # use to store the result of tracking object on the image\n",
    "    template_coords_result = [] # use to store the coordinate of boundary box for template object\n",
    "    groundtruth_data = np.array(groundtruth_data).reshape(-1,4) # reshaping ground truth data with 4 column matrix\n",
    "\n",
    "    # Initialize Kalman Filter Parameters\n",
    "    F = np.array([[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]]) # State Transition Matrix\n",
    "\n",
    "    H = np.array([[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]]) # Measurement Matrix\n",
    "\n",
    "    P = np.eye(4) * 1e2  # Covariance matrix initialization\n",
    "    R = np.eye(4) * 1e1  # Measurement noise covariance matrix (increasing value will make the filter not trust measurement so success and precision decrease)\n",
    "    Q = np.eye(4) * 1e1  # Process noise covariance matrix (increasing value will make the filter trust prediction more so success and precision may increase)\n",
    "\n",
    "    # looping through all sequence of images to do single object tracking\n",
    "    for i, image in enumerate(image_sequences):\n",
    "\n",
    "        measurement = groundtruth_data[i] # get measurement data from corresponding ground truth\n",
    "\n",
    "        predicted_state = kalman_filter_implementation(initial_state, F, H, P, R, Q, measurement) # calling kalman filter implementation function to give the result of predicted state\n",
    "\n",
    "        template_coords_result.append(predicted_state) # append the result of predicted state into template coords list (the boundary box of the target object)\n",
    "\n",
    "        current_image = image.copy()\n",
    "\n",
    "        cv2.rectangle(current_image, (int(predicted_state[0]), int(predicted_state[1])), (int(predicted_state[0]) + int(predicted_state[2]), int(predicted_state[1]) + int(predicted_state[3])), \n",
    "                      (255, 0, 0), 2) # draw tracked object boundary box in current image using cv2.rectangle\n",
    "        \n",
    "        cv2.putText(current_image, \"Kalman Filter\", (int(predicted_state[0]), int(predicted_state[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2) # put text name of \"Kalman Filter\" in the upper of boundary box of tracked object\n",
    "\n",
    "        cv2.rectangle(current_image,(groundtruth_data[i,0],groundtruth_data[i,1]),(groundtruth_data[i,0] + groundtruth_data[i,2], groundtruth_data[i,1] + groundtruth_data[i,3]),\n",
    "                        (0, 0, 255), 2) # draw ground truth boundary box in current image using cv2.rectangle\n",
    "        \n",
    "        cv2.putText(current_image, \"Ground Truth\", (groundtruth_data[i,0],groundtruth_data[i,1]-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2) # put text name of \"Ground Truth\" in the upper of boundary box of ground truth\n",
    "        \n",
    "        image_results.append(current_image) # append the result image that have boundary box of target object\n",
    "    \n",
    "    return image_results, template_coords_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the Single object tracking algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.5811388300841898\n",
      "4.123105625617661\n",
      "7.0178344238090995\n",
      "6.576473218982953\n",
      "5.024937810560445\n",
      "2.692582403567252\n",
      "2.23606797749979\n",
      "2.0615528128088303\n",
      "2.5\n",
      "2.9154759474226504\n",
      "3.3541019662496847\n",
      "2.5495097567963922\n",
      "3.3541019662496847\n",
      "3.5355339059327378\n",
      "3.5\n",
      "4.6097722286464435\n",
      "4.6097722286464435\n",
      "7.810249675906654\n",
      "10.307764064044152\n",
      "10.307764064044152\n",
      "10.735455276791944\n",
      "13.46291201783626\n",
      "12.36931687685298\n",
      "14.713938969562161\n",
      "15.508062419270823\n",
      "14.5\n",
      "16.62077013859466\n",
      "30.0\n",
      "31.48412298286233\n",
      "33.94849039353591\n",
      "34.007352146263905\n",
      "35.04639781775011\n",
      "39.2587569849072\n",
      "41.548164821084455\n",
      "38.99358921669048\n",
      "42.32316150761897\n",
      "40.74616546375867\n",
      "39.38591118661596\n",
      "38.91336531321854\n",
      "38.46101922726437\n",
      "40.65095324835569\n",
      "40.99390198553927\n",
      "42.87773314903669\n",
      "42.02975136733502\n",
      "41.743262929483606\n",
      "42.573465914816005\n",
      "34.61574786134195\n",
      "34.23448553724738\n",
      "32.687918257362305\n",
      "0.0\n",
      "0.7071067811865476\n",
      "2.692582403567252\n",
      "2.692582403567252\n",
      "2.0\n",
      "2.23606797749979\n",
      "2.1213203435596424\n",
      "2.9154759474226504\n",
      "2.5\n",
      "6.020797289396148\n",
      "23.50531854708632\n",
      "23.264780248263683\n",
      "23.942639787625758\n",
      "25.739075352467502\n",
      "25.739075352467502\n",
      "33.97793401606401\n",
      "40.10610926031095\n",
      "44.90545623863541\n",
      "44.9694340635948\n",
      "54.09482415166907\n",
      "58.89821729050889\n",
      "62.13895718468407\n",
      "63.657285521768834\n",
      "64.20669746996803\n",
      "65.59344479443048\n",
      "66.72705598181295\n",
      "65.94126174103738\n",
      "65.25526798657714\n",
      "63.547226532713445\n",
      "65.77423507727019\n",
      "67.15094936037762\n",
      "67.26440366196671\n",
      "68.6476510887299\n",
      "69.06518659932803\n",
      "71.84705978674423\n",
      "70.80254232723568\n",
      "71.40028011149536\n",
      "73.40980860893181\n",
      "74.81310045707235\n",
      "75.73803535872844\n",
      "76.42807075937479\n",
      "75.89631084578485\n",
      "77.27386362800814\n",
      "77.43545699484184\n",
      "77.27386362800814\n",
      "76.42807075937479\n",
      "74.05572226371167\n",
      "73.91380114701178\n",
      "74.34547733386343\n",
      "72.94518489934754\n",
      "Template Matching Performance for seq_5 --> Precision: 0.38, Success: 0.43, Normalized Precision: 0.32899910345073485\n",
      "0.0\n",
      "0.13176438395416679\n",
      "0.34358781788813614\n",
      "0.5017290947386239\n",
      "0.38415196126489426\n",
      "0.09316381244755823\n",
      "0.3033345936756067\n",
      "0.5833396912120011\n",
      "0.584819173075697\n",
      "1.0008710670776586\n",
      "1.1257750584700978\n",
      "1.37752597643816\n",
      "1.7088476817989964\n",
      "2.1266366727318053\n",
      "2.2087250608033124\n",
      "2.3366778790566185\n",
      "2.9583396911719473\n",
      "3.2502669526523644\n",
      "3.8333282470779046\n",
      "4.461439090785218\n",
      "4.461439090785218\n",
      "4.87784479285762\n",
      "5.544174019105566\n",
      "6.014447267314051\n",
      "6.765156616881744\n",
      "7.408936704914181\n",
      "7.322081856976393\n",
      "8.00184420019009\n",
      "8.988026306945848\n",
      "9.64409067493986\n",
      "10.298478655944797\n",
      "10.762741586121853\n",
      "10.823141164846888\n",
      "11.5009621434387\n",
      "12.09661354250036\n",
      "12.19871885820737\n",
      "12.90757276292194\n",
      "13.412840512852627\n",
      "13.316440590080012\n",
      "13.713907907490876\n",
      "14.11294120066926\n",
      "14.583673386031135\n",
      "14.981123635927078\n",
      "15.4192460832079\n",
      "15.337048795697612\n",
      "15.59745695588416\n",
      "15.926144589092742\n",
      "15.879352597003951\n",
      "16.03120002963192\n",
      "16.070247190317232\n",
      "16.155958958861888\n",
      "16.3614024942674\n",
      "16.310870013251684\n",
      "16.275813074580345\n",
      "16.483821386747845\n",
      "16.327646118479898\n",
      "16.28889523061403\n",
      "16.218175539383605\n",
      "16.215240869836528\n",
      "16.088387368911587\n",
      "15.718478750535573\n",
      "15.840436292549084\n",
      "15.808949718228961\n",
      "16.018485569828147\n",
      "16.09957439947681\n",
      "15.637210872619988\n",
      "15.329309025245509\n",
      "15.086845480817471\n",
      "14.837188187671092\n",
      "14.289173861569779\n",
      "13.859714397346412\n",
      "13.39631082052985\n",
      "13.372052304626644\n",
      "13.357330685388861\n",
      "13.30914736436029\n",
      "13.279369018606095\n",
      "13.392034708940217\n",
      "13.573982468237473\n",
      "13.88856611428155\n",
      "13.916042900192569\n",
      "13.865543721049258\n",
      "14.011444855786618\n",
      "13.962065650366007\n",
      "14.04510133811952\n",
      "13.950360935581658\n",
      "14.136044159669881\n",
      "14.167200751637456\n",
      "14.153974816676199\n",
      "14.109989296059133\n",
      "13.471280357153416\n",
      "13.448698541655522\n",
      "13.339836140219706\n",
      "13.294985437574095\n",
      "13.163613077992098\n",
      "13.141567338076987\n",
      "13.141567338076987\n",
      "13.240420395321326\n",
      "13.218240033183681\n",
      "13.305825798587732\n",
      "13.147445035839018\n",
      "Kalman Filter Performance for seq_5 --> Precision: 1.0, Success: 1.0, Normalized Precision: 0.1043159223616525\n"
     ]
    }
   ],
   "source": [
    "## Metric Evaluation of performance for single object tracking algorithm\n",
    "\n",
    "# function to calculate the distance between the center coordinate of boundary box of template with the center coordinate4 of boundary box of ground truth\n",
    "def calculate_distance(box1, box2):\n",
    "    center1 = (box1[0] + box1[2] / 2, box1[1] + box1[3] / 2)\n",
    "    center2 = (box2[0] + box2[2] / 2, box2[1] + box2[3] / 2)\n",
    "    distance = np.sqrt((center1[0] - center2[0]) ** 2 + (center1[1] - center2[1]) ** 2)\n",
    "    return distance\n",
    "\n",
    "# function to calculate success by computing IOU\n",
    "def calculate_iou(box1, box2):\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[0] + box1[2], box2[0] + box2[2])\n",
    "    y2 = min(box1[1] + box1[3], box2[1] + box2[3])\n",
    "    overlap_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    union_area = box1[2] * box1[3] + box2[2] * box2[3] - overlap_area\n",
    "    iou = overlap_area / union_area\n",
    "    return iou\n",
    "\n",
    "# function to evaluate the performance for template coordinate (target boundary box) with respect to ground truth boundary box\n",
    "def evaluate_performance(template_coords, ground_truth_coords):\n",
    "    distances = [] # to store distance value\n",
    "    ious = [] # to store iou value\n",
    "    pnorms = [] # to store normalize precision value\n",
    "\n",
    "    # loop for every template coordinates and corresponding ground truths in each frame of sequence images\n",
    "    for target, groundtruth in zip(template_coords, ground_truth_coords):\n",
    "        distance = calculate_distance(target, groundtruth) # calling function calculate_distance to compute center of boundary box of the target object with center of boundary box of ground truth\n",
    "        iou = calculate_iou(target, groundtruth) # calling function calculate_iou to compute IOU where overlap area of boundary box divided by union area\n",
    "        pnorm = distance / np.sqrt(groundtruth[2] * groundtruth[3]) # calling function to normalize precision (distance) by dividing with the square root of the size of width and height of ground truth boundary box\n",
    "\n",
    "        distances.append(distance)\n",
    "        ious.append(iou)\n",
    "        pnorms.append(pnorm)\n",
    "\n",
    "    precision_threshold = 20 # set precision threshold as 20 pixels\n",
    "    precision = np.mean(np.array(distances) < precision_threshold) # calculate precision based on the distance value below precision threshold and find the average from this\n",
    "    success = np.mean(np.array(ious) > 0.5)  # calculate success based on iou value above iou threshold (set to 0.5) and find the average\n",
    "\n",
    "    return precision, success, np.mean(pnorms)\n",
    "\n",
    "# Evaluate performance for template matching algorithm\n",
    "result_images, result_template_coords = template_matching_algorithm(image_sequences,template_coords,ground_truth_coords) # calling function that implement object tracking using template matching algorithm\n",
    "precision, success, pnorm = evaluate_performance(result_template_coords, ground_truth_coords) # calling function to evaluate the performance of this algorithm from template coordinates vs ground truth coordinates\n",
    "print(f\"Template Matching Performance for seq_{seq_slider.value} --> Precision: {precision}, Success: {success}, Normalized Precision: {pnorm}\")\n",
    "\n",
    "# Evaluate performance for Kalman Filter Algorithm\n",
    "result_images, result_template_coords = kalman_filter_algorithm(template_coords,image_sequences, ground_truth_coords) # calling function that implement object tracking using kalman filter algorithm\n",
    "precision, success, pnorm = evaluate_performance(result_template_coords, ground_truth_coords) # calling function to evaluate the performance of this algorithm from template coordinates vs ground truth coordinates\n",
    "print(f\"Kalman Filter Performance for seq_{seq_slider.value} --> Precision: {precision}, Success: {success}, Normalized Precision: {pnorm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Visualise the results of object tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization the result of single object tracking algorithm\n",
    "\n",
    "# Visulaization result and saving location of tracked object in each frame for Template Matching Algorithm\n",
    "result_images, result_template_coords = template_matching_algorithm(image_sequences,template_coords,ground_truth_coords) # calling function that implement object tracking using template matching algorithm\n",
    "\n",
    "#result_template_coords_array = np.array(result_template_coords)\n",
    "np.savetxt(os.path.join('result', f'result_template_coords_template_matching_seq{seq_slider.value}.txt'), result_template_coords, delimiter=',', fmt='%d') # use to save the boundary box coordinates of tracked object into txt file\n",
    "\n",
    "cv2.namedWindow(\"Single Object Tracking with template matching algorithm\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "for i, result_image in enumerate(result_images):\n",
    "    cv2.imshow(\"Single Object Tracking with template matching algorithm\", result_image)\n",
    "    if cv2.waitKey(75) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Visualization result and saving location of tracked object in each frame for Kalman Filter Algorithm\n",
    "result_images, result_template_coords = kalman_filter_algorithm(template_coords, image_sequences, ground_truth_coords)\n",
    "\n",
    "#result_template_coords_array = np.array(result_template_coords)\n",
    "np.savetxt(os.path.join('result', f'result_template_coords_kalman_filter_seq{seq_slider.value}.txt'), result_template_coords, delimiter=',', fmt='%d') # use to save the boundary box coordinates of tracked object into txt file\n",
    "\n",
    "cv2.namedWindow(\"Single Object Tracking with Kalman Filter Algorithm\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "for i, result_image in enumerate(result_images):\n",
    "    cv2.imshow(\"Single Object Tracking with Kalman Filter Algorithm\", result_image)\n",
    "    if cv2.waitKey(75) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
